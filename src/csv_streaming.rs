//! CSV streaming processing with memory-efficient batch operations
//!
//! This module provides streaming CSV processing capabilities using csv-async
//! and tokio-stream for processing large datasets without loading everything into memory.

use crate::range_bars::ExportRangeBarProcessor;
// #[cfg(feature = "streaming-stats")]
// use crate::streaming_stats::StreamingStats;
use crate::types::{AggTrade, RangeBar};
use serde::Deserialize;
use std::error::Error;
use std::path::Path;

/// Custom deserializer for Python-style booleans (True/False/true/false)
fn python_bool<'de, D>(deserializer: D) -> Result<bool, D::Error>
where
    D: serde::Deserializer<'de>,
{
    let s = String::deserialize(deserializer)?;
    match s.as_str() {
        "True" | "true" => Ok(true),
        "False" | "false" => Ok(false),
        _ => Err(serde::de::Error::custom(format!(
            "Invalid boolean value: {}",
            s
        ))),
    }
}

/// CSV trade record format for async deserialization
#[derive(Debug, Clone, Deserialize)]
pub struct CsvAggTrade {
    #[serde(rename = "a")]
    pub agg_trade_id: i64,
    #[serde(rename = "p")]
    pub price: String,
    #[serde(rename = "q")]
    pub volume: String,
    #[serde(rename = "f")]
    pub first_trade_id: i64,
    #[serde(rename = "l")]
    pub last_trade_id: i64,
    #[serde(rename = "T")]
    pub timestamp: i64,
    #[serde(rename = "m", deserialize_with = "python_bool")]
    pub is_buyer_maker: bool,
}

impl From<CsvAggTrade> for AggTrade {
    fn from(csv_trade: CsvAggTrade) -> Self {
        use crate::fixed_point::FixedPoint;

        Self {
            agg_trade_id: csv_trade.agg_trade_id,
            price: FixedPoint::from_str(&csv_trade.price).unwrap_or_else(|_| FixedPoint(0)),
            volume: FixedPoint::from_str(&csv_trade.volume).unwrap_or_else(|_| FixedPoint(0)),
            first_trade_id: csv_trade.first_trade_id,
            last_trade_id: csv_trade.last_trade_id,
            timestamp: csv_trade.timestamp,
            is_buyer_maker: csv_trade.is_buyer_maker,
        }
    }
}

/// Streaming CSV processor for memory-efficient range bar generation
pub struct StreamingCsvProcessor {
    /// Batch size for processing trades
    batch_size: usize,
    /// Enable debug logging
    debug: bool,
}

impl Default for StreamingCsvProcessor {
    fn default() -> Self {
        Self::new()
    }
}

impl StreamingCsvProcessor {
    /// Create new streaming CSV processor with default batch size
    pub fn new() -> Self {
        Self {
            batch_size: 10_000, // 10K trades per batch for optimal performance
            debug: false,
        }
    }

    /// Create processor with custom batch size
    pub fn with_batch_size(batch_size: usize) -> Self {
        Self {
            batch_size,
            debug: false,
        }
    }

    /// Enable debug logging
    pub fn with_debug(mut self) -> Self {
        self.debug = true;
        self
    }

    /// Read CSV file in batches (simplified approach)
    pub async fn read_csv_in_batches<P: AsRef<Path>>(
        &self,
        file_path: P,
    ) -> Result<Vec<AggTrade>, Box<dyn Error + Send + Sync>> {
        use csv::ReaderBuilder;
        use std::io::BufReader;

        let file = std::fs::File::open(file_path)?;
        let buf_reader = BufReader::new(file);
        let mut csv_reader = ReaderBuilder::new()
            .has_headers(true)
            .from_reader(buf_reader);

        let mut trades = Vec::new();
        for result in csv_reader.deserialize() {
            let csv_trade: CsvAggTrade = result?;
            trades.push(AggTrade::from(csv_trade));
        }

        Ok(trades)
    }

    /// Process CSV file with streaming statistics (memory-efficient)
    // #[cfg(feature = "streaming-stats")]
    // pub async fn process_csv_with_streaming_stats<P: AsRef<Path>>(
    //     &self,
    //     file_path: P,
    //     processor: &mut ExportRangeBarProcessor,
    // ) -> Result<(Vec<RangeBar>, StreamingStats), Box<dyn Error + Send + Sync>> {
    //     let mut trade_stream = self.stream_csv_trades(file_path).await?;
    //     let mut streaming_stats = StreamingStats::new();
        let mut processed_count = 0u64;

        if self.debug {
            println!(
                "ðŸ“Š Starting streaming CSV processing with batch size: {}",
                self.batch_size
            );
        }

        // Process in batches using chunks from tokio-stream
        let mut batch = Vec::with_capacity(self.batch_size);

        while let Some(result) = futures::StreamExt::next(&mut trade_stream).await {
            let trade = result?;

            // Update streaming statistics
            streaming_stats.update(&trade);

            // Add to batch
            batch.push(trade);

            // Process batch when full
            if batch.len() >= self.batch_size {
                processor.process_trades_continuously(&batch);
                processed_count += batch.len() as u64;

                if self.debug && processed_count % 100_000 == 0 {
                    println!(
                        "   ðŸ“ˆ Processed {} trades, memory usage: ~{}KB",
                        processed_count,
                        streaming_stats.memory_usage_bytes() / 1024
                    );
                }

                batch.clear(); // Free memory immediately
            }
        }

        // Process remaining trades in final batch
        if !batch.is_empty() {
            processor.process_trades_continuously(&batch);
            processed_count += batch.len() as u64;
        }

        let range_bars = processor.get_all_completed_bars();

        if self.debug {
            println!(
                "âœ… Streaming processing complete: {} trades â†’ {} range bars",
                processed_count,
                range_bars.len()
            );
            println!(
                "   ðŸ’¾ Memory usage: ~{}KB (vs ~{}GB for in-memory)",
                streaming_stats.memory_usage_bytes() / 1024,
                (processed_count * 80) / 1_000_000
            ); // Estimate: 80 bytes per trade
        }

        Ok((range_bars, streaming_stats))
    }

    /// Process CSV file with batch processing only (no statistics)
    pub async fn process_csv_batched<P: AsRef<Path>>(
        &self,
        file_path: P,
        processor: &mut ExportRangeBarProcessor,
    ) -> Result<Vec<RangeBar>, Box<dyn Error + Send + Sync>> {
        if self.debug {
            println!("âš¡ Starting batched CSV processing (no statistics)");
        }

        // Read all trades at once (simplified for now)
        let all_trades = self.read_csv_in_batches(file_path).await?;
        let processed_count = all_trades.len();

        if self.debug {
            println!("   ðŸ“„ Loaded {} trades from CSV", processed_count);
        }

        // Process in batches
        let mut range_bars = Vec::new();
        for batch in all_trades.chunks(self.batch_size) {
            processor.process_trades_continuously(batch);
            // Get completed bars from this batch and clear state for next batch
            let batch_bars = processor.get_all_completed_bars();
            range_bars.extend(batch_bars);

            if self.debug && processed_count % 100_000 == 0 {
                println!("   âš¡ Processed batch");
            }
        }

        if self.debug {
            println!(
                "âœ… Batched processing complete: {} trades â†’ {} range bars",
                processed_count,
                range_bars.len()
            );
        }

        Ok(range_bars)
    }

    /// Process multiple CSV files in sequence with streaming statistics
    #[cfg(feature = "streaming-stats")]
    pub async fn process_multiple_csv_files<P: AsRef<Path>>(
        &self,
        file_paths: &[P],
        processor: &mut ExportRangeBarProcessor,
    ) -> Result<(Vec<RangeBar>, StreamingStats), Box<dyn Error + Send + Sync>> {
        let mut combined_stats = StreamingStats::new();
        let mut _total_processed = 0u64;

        if self.debug {
            println!(
                "ðŸ“‚ Processing {} CSV files with streaming statistics",
                file_paths.len()
            );
        }

        for (index, file_path) in file_paths.iter().enumerate() {
            if self.debug {
                println!("   ðŸ“„ Processing file {}/{}", index + 1, file_paths.len());
            }

            let (_bars, file_stats) = self
                .process_csv_with_streaming_stats(file_path, processor)
                .await?;

            // Merge statistics from this file
            combined_stats = combined_stats.merge(file_stats);
            total_processed += combined_stats.trade_count;

            if self.debug {
                println!(
                    "   âœ… File completed, cumulative trades: {}",
                    combined_stats.trade_count
                );
            }
        }

        let final_range_bars = processor.get_all_completed_bars();

        if self.debug {
            println!(
                "ðŸŽ‰ All files processed: {} total trades â†’ {} range bars",
                combined_stats.trade_count,
                final_range_bars.len()
            );
            println!(
                "   ðŸ“Š Final memory usage: ~{}KB",
                combined_stats.memory_usage_bytes() / 1024
            );
        }

        Ok((final_range_bars, combined_stats))
    }

    /// Get optimal batch size based on available memory
    pub fn optimize_batch_size_for_memory(target_memory_mb: usize) -> usize {
        // Estimate: AggTrade â‰ˆ 80 bytes, processing overhead â‰ˆ 2x
        let bytes_per_trade = 80 * 2;
        let target_bytes = target_memory_mb * 1_024 * 1_024;
        let optimal_batch = target_bytes / bytes_per_trade;

        // Clamp to reasonable range
        optimal_batch.max(1_000).min(1_000_000)
    }

    /// Create processor optimized for low memory usage (< 50MB)
    pub fn low_memory() -> Self {
        Self::with_batch_size(Self::optimize_batch_size_for_memory(10)) // 10MB target
    }

    /// Create processor optimized for balanced performance (~200MB)
    pub fn balanced() -> Self {
        Self::with_batch_size(Self::optimize_batch_size_for_memory(50)) // 50MB target
    }

    /// Create processor optimized for high performance (~500MB)
    pub fn high_performance() -> Self {
        Self::with_batch_size(Self::optimize_batch_size_for_memory(100)) // 100MB target
    }
}

/// Utility functions for streaming CSV processing
pub mod utils {
    use super::*;
    use std::path::PathBuf;
    use tokio::fs;

    /// Check if a CSV file exists and is readable
    pub async fn validate_csv_file<P: AsRef<Path>>(
        file_path: P,
    ) -> Result<u64, Box<dyn Error + Send + Sync>> {
        let metadata = fs::metadata(&file_path).await?;
        Ok(metadata.len())
    }

    /// Estimate processing time based on file size and system specs
    pub fn estimate_processing_time(
        file_size_bytes: u64,
        _batch_size: usize,
    ) -> std::time::Duration {
        // Rough estimates based on SSD I/O and processing speed
        let bytes_per_second = 50_000_000; // 50MB/s typical SSD throughput
        let trades_per_second = 500_000; // Estimated processing rate

        let io_time_secs = file_size_bytes / bytes_per_second;
        let estimated_trades = file_size_bytes / 100; // ~100 bytes per trade line
        let processing_time_secs = estimated_trades / trades_per_second;

        let total_secs = io_time_secs.max(processing_time_secs);
        std::time::Duration::from_secs(total_secs + 1) // Add 1 second buffer
    }

    /// Get file paths matching a pattern (e.g., "BTCUSDT_*.csv")
    pub async fn find_csv_files<P: AsRef<Path>>(
        directory: P,
        pattern: &str,
    ) -> Result<Vec<PathBuf>, Box<dyn Error + Send + Sync>> {
        let mut matching_files = Vec::new();
        let mut entries = fs::read_dir(directory).await?;

        while let Some(entry) = entries.next_entry().await? {
            let path = entry.path();
            if let Some(file_name) = path.file_name().and_then(|n| n.to_str()) {
                if file_name.contains(pattern) && file_name.ends_with(".csv") {
                    matching_files.push(path);
                }
            }
        }

        // Sort files for deterministic processing order
        matching_files.sort();
        Ok(matching_files)
    }
}

// TODO: Fix CSV streaming tests after API changes
/*
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::NamedTempFile;
    use tokio::io::AsyncWriteExt;

    async fn create_test_csv() -> Result<NamedTempFile, Box<dyn Error + Send + Sync>> {
        let mut temp_file = NamedTempFile::new()?;

        // Write CSV header and test data
        let csv_content = r#"a,p,q,f,l,T,m
123456789,50000.12345,1.50000000,100,105,1609459200000,false
123456790,50100.67890,2.25000000,106,110,1609459201000,true
123456791,49900.55555,1.75000000,111,115,1609459202000,false
"#;

        temp_file.write_all(csv_content.as_bytes()).await?;
        temp_file.flush().await?;

        Ok(temp_file)
    }

    #[tokio::test]
    async fn test_csv_streaming_basic() {
        let temp_file = create_test_csv().await.unwrap();
        let processor = StreamingCsvProcessor::new();

        let mut trade_stream = processor.stream_csv_trades(temp_file.path()).await.unwrap();
        let mut trade_count = 0;

        while let Some(result) = futures::StreamExt::next(&mut trade_stream).await {
            let trade = result.unwrap();
            assert!(trade.price.0 > 0);
            assert!(trade.volume.0 > 0);
            trade_count += 1;
        }

        assert_eq!(trade_count, 3);
    }

    #[tokio::test]
    async fn test_batch_size_optimization() {
        // Test low memory optimization
        let low_mem_batch = StreamingCsvProcessor::optimize_batch_size_for_memory(10);
        assert!(low_mem_batch >= 1_000 && low_mem_batch <= 100_000);

        // Test high performance optimization
        let high_perf_batch = StreamingCsvProcessor::optimize_batch_size_for_memory(100);
        assert!(high_perf_batch > low_mem_batch);
    }

    #[test]
    fn test_processor_presets() {
        let low_mem = StreamingCsvProcessor::low_memory();
        let balanced = StreamingCsvProcessor::balanced();
        let high_perf = StreamingCsvProcessor::high_performance();

        assert!(low_mem.batch_size < balanced.batch_size);
        assert!(balanced.batch_size < high_perf.batch_size);
    }

    #[tokio::test]
    async fn test_file_validation() {
        let temp_file = create_test_csv().await.unwrap();
        let file_size = utils::validate_csv_file(temp_file.path()).await.unwrap();
        assert!(file_size > 0);

        // Test non-existent file
        let result = utils::validate_csv_file("/nonexistent/file.csv").await;
        assert!(result.is_err());
    }
}
*/
